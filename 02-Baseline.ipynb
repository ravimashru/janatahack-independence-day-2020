{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-390c353e-3f32-45fd-b67e-eca3c584a7c1"},"source":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport re","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-91d47026-9f30-4b63-b9dd-fdcff42a9234"},"source":"print('pandas: ', pd.__version__)\nprint('matplotlib: ', matplotlib.__version__)","outputs":[{"name":"stdout","text":"pandas:  1.0.5\nmatplotlib:  3.2.2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-bf9cb6f4-47a2-46d8-9d7d-b898d03c0c0f"},"source":"df_train = pd.read_csv('./data/train.csv')\ndf_train.head()","outputs":[{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":9,"columns":[{"name":"ID","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":1,"max":5,"histogram":[{"bin_start":1,"bin_end":1.4,"count":1},{"bin_start":1.4,"bin_end":1.8,"count":0},{"bin_start":1.8,"bin_end":2.2,"count":1},{"bin_start":2.2,"bin_end":2.6,"count":0},{"bin_start":2.6,"bin_end":3,"count":0},{"bin_start":3,"bin_end":3.4000000000000004,"count":1},{"bin_start":3.4000000000000004,"bin_end":3.8000000000000003,"count":0},{"bin_start":3.8000000000000003,"bin_end":4.2,"count":1},{"bin_start":4.2,"bin_end":4.6,"count":0},{"bin_start":4.6,"bin_end":5,"count":1}]}},{"name":"TITLE","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Reconstructing Subject-Specific Effect Maps","count":1},{"name":"Rotation Invariance Neural Network","count":1},{"name":"3 others","count":3}]}},{"name":"ABSTRACT","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"  Predictive models allow subject-specific inference when analyzing disease\nrelated alterations in neuroimaging data. Given a subject's data, inference can\nbe made at two levels: global, i.e. identifiying condition presence for the\nsubject, and local, i.e. detecting condition effect on each individual\nmeasurement extracted from the subject's data. While global inference is widely\nused, local inference, which can be used to form subject-specific effect maps,\nis rarely used because existing models often yield noisy detections composed of\ndispersed isolated islands. In this article, we propose a reconstruction\nmethod, named RSM, to improve subject-specific detections of predictive\nmodeling approaches and in particular, binary classifiers. RSM specifically\naims to reduce noise due to sampling error associated with using a finite\nsample of examples to train classifiers. The proposed method is a wrapper-type\nalgorithm that can be used with different binary classifiers in a diagnostic\nmanner, i.e. without information on condition presence. Reconstruction is posed\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\nestimated from training data in a classifier-specific fashion. Experimental\nevaluation is performed on synthetically generated data and data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\nsynthetic data demonstrate that using RSM yields higher detection accuracy\ncompared to using models directly or with bootstrap averaging. Analyses on the\nADNI dataset show that RSM can also improve correlation between\nsubject-specific detections in cortical thickness data and non-imaging markers\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\nthe longitudinal ADNI dataset show improvement on detection reliability when\nRSM is used.\n","count":1},{"name":"  Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n","count":1},{"name":"3 others","count":3}]}},{"name":"Computer Science","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":3}]}},{"name":"Physics","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Mathematics","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"Statistics","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"Quantitative Biology","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Quantitative Finance","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"ID":1,"TITLE":"Reconstructing Subject-Specific Effect Maps","ABSTRACT":"  Predictive models allow subject-specific inference when analyzing disease\nrelated alterations in neuroimaging data. Given a subject's data, inference can\nbe made at two levels: global, i.e. identifiying condition presence for the\nsubject, and local, i.e. detecting condition effect on each individual\nmeasurement extracted from the subject's data. While global inference is widely\nused, local inference, which can be used to form subject-specific effect maps,\nis rarely used because existing models often yield noisy detections composed of\ndispersed isolated islands. In this article, we propose a reconstruction\nmethod, named RSM, to improve subject-specific detections of predictive\nmodeling approaches and in particular, binary classifiers. RSM specifically\naims to reduce noise due to sampling error associated with using a finite\nsample of examples to train classifiers. The proposed method is a wrapper-type\nalgorithm that can be used with different binary classifiers in a diagnostic\nmanner, i.e. without information on condition presence. Reconstruction is posed\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\nestimated from training data in a classifier-specific fashion. Experimental\nevaluation is performed on synthetically generated data and data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\nsynthetic data demonstrate that using RSM yields higher detection accuracy\ncompared to using models directly or with bootstrap averaging. Analyses on the\nADNI dataset show that RSM can also improve correlation between\nsubject-specific detections in cortical thickness data and non-imaging markers\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\nthe longitudinal ADNI dataset show improvement on detection reliability when\nRSM is used.\n","Computer Science":1,"Physics":0,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":0},{"ID":2,"TITLE":"Rotation Invariance Neural Network","ABSTRACT":"  Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n","Computer Science":1,"Physics":0,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":1},{"ID":3,"TITLE":"Spherical polyharmonics and Poisson kernels for polyharmonic functions","ABSTRACT":"  We introduce and develop the notion of spherical polyharmonics, which are a\nnatural generalisation of spherical harmonics. In particular we study the\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\nto construct Poisson kernels for polyharmonic functions on the union of rotated\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\nterms of the Gegenbauer polynomials. We show the connection between the\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\nkernel for holomorphic functions on the Lie ball.\n","Computer Science":0,"Physics":0,"Mathematics":1,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":2},{"ID":4,"TITLE":"A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system","ABSTRACT":"  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\nMaxwell equations (the so called stochastic MLLG system) describes the creation\nof domain walls and vortices (fundamental objects for the novel nanostructured\nmagnetic memories). We first reformulate the stochastic LLG equation into an\nequation with time-differentiable solutions. We then propose a convergent\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\nAs a consequence, we prove convergence of the approximate solutions, with no or\nminor conditions on time and space steps (depending on the value of $\\theta$).\nHence, we prove the existence of weak martingale solutions of the stochastic\nMLLG system. Numerical results are presented to show applicability of the\nmethod.\n","Computer Science":0,"Physics":0,"Mathematics":1,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":3},{"ID":5,"TITLE":"Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants","ABSTRACT":"  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\nwere used to explore the influence of preprocessing and feature extraction on\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\ntechniques for FTIR data of medicinal plants. Various combinations of signal\nprocessing steps showed different behavior when applied to classification and\nclustering tasks. Best results for WTT and DWT found through grid search were\nsimilar, significantly improving quality of clustering as well as\nclassification accuracy for tuned logistic regression in comparison to original\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\nmore versatile and easier to use as a data processing tool in various signal\nprocessing applications.\n","Computer Science":1,"Physics":0,"Mathematics":0,"Statistics":1,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"   ID                                              TITLE  \\\n0   1        Reconstructing Subject-Specific Effect Maps   \n1   2                 Rotation Invariance Neural Network   \n2   3  Spherical polyharmonics and Poisson kernels fo...   \n3   4  A finite element approximation for the stochas...   \n4   5  Comparative study of Discrete Wavelet Transfor...   \n\n                                            ABSTRACT  Computer Science  \\\n0    Predictive models allow subject-specific inf...                 1   \n1    Rotation invariance and translation invarian...                 1   \n2    We introduce and develop the notion of spher...                 0   \n3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n\n   Physics  Mathematics  Statistics  Quantitative Biology  \\\n0        0            0           0                     0   \n1        0            0           0                     0   \n2        0            1           0                     0   \n3        0            1           0                     0   \n4        0            0           1                     0   \n\n   Quantitative Finance  \n0                     0  \n1                     0  \n2                     0  \n3                     0  \n4                     0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>ABSTRACT</th>\n      <th>Computer Science</th>\n      <th>Physics</th>\n      <th>Mathematics</th>\n      <th>Statistics</th>\n      <th>Quantitative Biology</th>\n      <th>Quantitative Finance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Reconstructing Subject-Specific Effect Maps</td>\n      <td>Predictive models allow subject-specific inf...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Rotation Invariance Neural Network</td>\n      <td>Rotation invariance and translation invarian...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n      <td>We introduce and develop the notion of spher...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>A finite element approximation for the stochas...</td>\n      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Comparative study of Discrete Wavelet Transfor...</td>\n      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-702b4925-091f-4b89-9928-a27cab59f6a6"},"source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-10ebfd54-2bab-4280-896f-e1ad7376af61"},"source":"df_train['TITLE_PROCESSED'] = df_train['TITLE'].apply(lambda text: clean_text(text))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-ac01c96c-d2b7-46a7-b6ca-42058696a36a"},"source":"df_train['ABSTRACT_PROCESSED'] = df_train['ABSTRACT'].apply(lambda text: clean_text(text))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-b4721a92-a38b-4b2b-8430-4f6f534fc589"},"source":"from sklearn.model_selection import train_test_split\n\ncategories = df_train.columns[3:-2].values\ntrain, test = train_test_split(df_train, random_state=42, test_size=0.33, shuffle=True)\nX_train = train[['TITLE_PROCESSED', 'ABSTRACT_PROCESSED']]\nX_test = test[['TITLE_PROCESSED', 'ABSTRACT_PROCESSED']]\nprint(X_train.shape)\nprint(X_test.shape)","outputs":[{"name":"stdout","text":"(14051, 2)\n(6921, 2)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-a54ac438-0aa7-4205-8e46-470af33b34fd"},"source":"import nltk\nnltk.download('stopwords')","outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-3d446c23-9e6a-493e-9a13-cb5d84070cef"},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-b64d03c7-4197-49b4-af40-8cafe703e2e7"},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\n\npreprocessing = FeatureUnion([\n                ('title_tfidf', \n                  Pipeline([('extract_field',\n                              FunctionTransformer(lambda x: x['TITLE_PROCESSED'], \n                                                  validate=False)),\n                            ('tfidf', \n                              TfidfVectorizer(stop_words=stop_words))])),\n                ('abstract_tfidf', \n                  Pipeline([('extract_field', \n                              FunctionTransformer(lambda x: x['ABSTRACT_PROCESSED'], \n                                                  validate=False)),\n                            ('tfidf', \n                              TfidfVectorizer(stop_words=stop_words))]))]) \nNB_pipeline = Pipeline([\n                ('preprocessing', preprocessing),\n                ('clf',\n                  OneVsRestClassifier(LogisticRegression())\n                ),\n            ])\n\nNB_pipeline.fit(X_train, train[categories])","outputs":[{"output_type":"execute_result","execution_count":100,"data":{"text/plain":"Pipeline(steps=[('preprocessing',\n                 FeatureUnion(transformer_list=[('title_tfidf',\n                                                 Pipeline(steps=[('extract_field',\n                                                                  FunctionTransformer(func=<function <lambda> at 0x7f712485e048>)),\n                                                                 ('tfidf',\n                                                                  TfidfVectorizer(stop_words={'a',\n                                                                                              'about',\n                                                                                              'above',\n                                                                                              'after',\n                                                                                              'again',\n                                                                                              'against',\n                                                                                              'ain',\n                                                                                              'all',\n                                                                                              'am',\n                                                                                              'an',\n                                                                                              'and',\n                                                                                              'any',\n                                                                                              'are',\n                                                                                              'aren',\n                                                                                              \"aren't\",\n                                                                                              'as',\n                                                                                              'at',\n                                                                                              'be',\n                                                                                              'because',\n                                                                                              'been',\n                                                                                              'befo...\n                                                                  FunctionTransformer(func=<function <lambda> at 0x7f712485e0d0>)),\n                                                                 ('tfidf',\n                                                                  TfidfVectorizer(stop_words={'a',\n                                                                                              'about',\n                                                                                              'above',\n                                                                                              'after',\n                                                                                              'again',\n                                                                                              'against',\n                                                                                              'ain',\n                                                                                              'all',\n                                                                                              'am',\n                                                                                              'an',\n                                                                                              'and',\n                                                                                              'any',\n                                                                                              'are',\n                                                                                              'aren',\n                                                                                              \"aren't\",\n                                                                                              'as',\n                                                                                              'at',\n                                                                                              'be',\n                                                                                              'because',\n                                                                                              'been',\n                                                                                              'before',\n                                                                                              'being',\n                                                                                              'below',\n                                                                                              'between',\n                                                                                              'both',\n                                                                                              'but',\n                                                                                              'by',\n                                                                                              'can',\n                                                                                              'couldn',\n                                                                                              \"couldn't\", ...}))]))])),\n                ('clf', OneVsRestClassifier(estimator=LogisticRegression()))])"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-174c7033-3b72-41a8-ba2a-3d75ae9157c1"},"source":"prediction = NB_pipeline.predict(X_test)\nf1_score(test[categories], prediction, average='micro')","outputs":[{"output_type":"execute_result","execution_count":101,"data":{"text/plain":"0.8016939790093905"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-486eb1a8-9e96-445a-852b-8b96e9014547"},"source":"# Train on all data and make predictions\nX = df_train[['TITLE_PROCESSED', 'ABSTRACT_PROCESSED']]\ny = df_train[categories]","outputs":[],"execution_count":null},{"cell_type":"code","source":"NB_pipeline.fit(X, y)","metadata":{"tags":[],"cell_id":"00014-44793030-4252-4397-9e74-cc3273774f9a"},"outputs":[{"output_type":"execute_result","execution_count":105,"data":{"text/plain":"Pipeline(steps=[('preprocessing',\n                 FeatureUnion(transformer_list=[('title_tfidf',\n                                                 Pipeline(steps=[('extract_field',\n                                                                  FunctionTransformer(func=<function <lambda> at 0x7f712485e048>)),\n                                                                 ('tfidf',\n                                                                  TfidfVectorizer(stop_words={'a',\n                                                                                              'about',\n                                                                                              'above',\n                                                                                              'after',\n                                                                                              'again',\n                                                                                              'against',\n                                                                                              'ain',\n                                                                                              'all',\n                                                                                              'am',\n                                                                                              'an',\n                                                                                              'and',\n                                                                                              'any',\n                                                                                              'are',\n                                                                                              'aren',\n                                                                                              \"aren't\",\n                                                                                              'as',\n                                                                                              'at',\n                                                                                              'be',\n                                                                                              'because',\n                                                                                              'been',\n                                                                                              'befo...\n                                                                  FunctionTransformer(func=<function <lambda> at 0x7f712485e0d0>)),\n                                                                 ('tfidf',\n                                                                  TfidfVectorizer(stop_words={'a',\n                                                                                              'about',\n                                                                                              'above',\n                                                                                              'after',\n                                                                                              'again',\n                                                                                              'against',\n                                                                                              'ain',\n                                                                                              'all',\n                                                                                              'am',\n                                                                                              'an',\n                                                                                              'and',\n                                                                                              'any',\n                                                                                              'are',\n                                                                                              'aren',\n                                                                                              \"aren't\",\n                                                                                              'as',\n                                                                                              'at',\n                                                                                              'be',\n                                                                                              'because',\n                                                                                              'been',\n                                                                                              'before',\n                                                                                              'being',\n                                                                                              'below',\n                                                                                              'between',\n                                                                                              'both',\n                                                                                              'but',\n                                                                                              'by',\n                                                                                              'can',\n                                                                                              'couldn',\n                                                                                              \"couldn't\", ...}))]))])),\n                ('clf', OneVsRestClassifier(estimator=LogisticRegression()))])"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv('./data/test.csv')\ndf_test['TITLE_PROCESSED'] = df_test['TITLE'].apply(lambda text: clean_text(text))\ndf_test['ABSTRACT_PROCESSED'] = df_test['ABSTRACT'].apply(lambda text: clean_text(text))\ndf_test.head()","metadata":{"tags":[],"cell_id":"00013-2b377766-fdb3-460a-85c1-063ddadf5b4b"},"outputs":[{"output_type":"execute_result","execution_count":107,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":5,"columns":[{"name":"ID","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":20973,"max":20977,"histogram":[{"bin_start":20973,"bin_end":20973.4,"count":1},{"bin_start":20973.4,"bin_end":20973.8,"count":0},{"bin_start":20973.8,"bin_end":20974.2,"count":1},{"bin_start":20974.2,"bin_end":20974.6,"count":0},{"bin_start":20974.6,"bin_end":20975,"count":0},{"bin_start":20975,"bin_end":20975.4,"count":1},{"bin_start":20975.4,"bin_end":20975.8,"count":0},{"bin_start":20975.8,"bin_end":20976.2,"count":1},{"bin_start":20976.2,"bin_end":20976.6,"count":0},{"bin_start":20976.6,"bin_end":20977,"count":1}]}},{"name":"TITLE","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization","count":1},{"name":"Laboratory mid-IR spectra of equilibrated and igneous meteorites. Searching for observables of planetesimal debris","count":1},{"name":"3 others","count":3}]}},{"name":"ABSTRACT","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"  We present novel understandings of the Gamma-Poisson (GaP) model, a\nprobabilistic matrix factorization model for count data. We show that GaP can\nbe rewritten free of the score/activation matrix. This gives us new insights\nabout the estimation of the topic/dictionary matrix by maximum marginal\nlikelihood estimation. In particular, this explains the robustness of this\nestimator to over-specified values of the factorization rank, especially its\nability to automatically prune irrelevant dictionary columns, as empirically\nobserved in previous work. The marginalization of the activation matrix leads\nin turn to a new Monte Carlo Expectation-Maximization algorithm with favorable\nproperties.\n","count":1},{"name":"  Meteorites contain minerals from Solar System asteroids with different\nproperties (like size, presence of water, core formation). We provide new\nmid-IR transmission spectra of powdered meteorites to obtain templates of how\nmid-IR spectra of asteroidal debris would look like. This is essential for\ninterpreting mid-IR spectra of past and future space observatories, like the\nJames Webb Space Telescope. We show that the transmission spectra of wet and\ndry chondrites, carbonaceous and ordinary chondrites and achondrite and\nchondrite meteorites are distinctly different in a way one can distinguish in\nastronomical mid-IR spectra. The two observables that spectroscopically\nseparate the different meteorites groups (and thus the different types of\nparent bodies) are the pyroxene-olivine feature strength ratio and the peak\nshift of the olivine spectral features due to an increase in the iron\nconcentration of the olivine.\n","count":1},{"name":"3 others","count":3}]}},{"name":"TITLE_PROCESSED","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"closed form marginal likelihood in gamma poisson matrix factorization","count":1},{"name":"laboratory mid ir spectra of equilibrated and igneous meteorites searching for observables of planetesimal debris","count":1},{"name":"3 others","count":3}]}},{"name":"ABSTRACT_PROCESSED","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"we present novel understandings of the gamma poisson gap model a probabilistic matrix factorization model for count data we show that gap can be rewritten free of the score activation matrix this gives us new insights about the estimation of the topic dictionary matrix by maximum marginal likelihood estimation in particular this explains the robustness of this estimator to over specified values of the factorization rank especially its ability to automatically prune irrelevant dictionary columns as empirically observed in previous work the marginalization of the activation matrix leads in turn to a new monte carlo expectation maximization algorithm with favorable properties","count":1},{"name":"meteorites contain minerals from solar system asteroids with different properties like size presence of water core formation we provide new mid ir transmission spectra of powdered meteorites to obtain templates of how mid ir spectra of asteroidal debris would look like this is essential for interpreting mid ir spectra of past and future space observatories like the james webb space telescope we show that the transmission spectra of wet and dry chondrites carbonaceous and ordinary chondrites and achondrite and chondrite meteorites are distinctly different in a way one can distinguish in astronomical mid ir spectra the two observables that spectroscopically separate the different meteorites groups and thus the different types of parent bodies are the pyroxene olivine feature strength ratio and the peak shift of the olivine spectral features due to an increase in the iron concentration of the olivine","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"ID":20973,"TITLE":"Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization","ABSTRACT":"  We present novel understandings of the Gamma-Poisson (GaP) model, a\nprobabilistic matrix factorization model for count data. We show that GaP can\nbe rewritten free of the score/activation matrix. This gives us new insights\nabout the estimation of the topic/dictionary matrix by maximum marginal\nlikelihood estimation. In particular, this explains the robustness of this\nestimator to over-specified values of the factorization rank, especially its\nability to automatically prune irrelevant dictionary columns, as empirically\nobserved in previous work. The marginalization of the activation matrix leads\nin turn to a new Monte Carlo Expectation-Maximization algorithm with favorable\nproperties.\n","TITLE_PROCESSED":"closed form marginal likelihood in gamma poisson matrix factorization","ABSTRACT_PROCESSED":"we present novel understandings of the gamma poisson gap model a probabilistic matrix factorization model for count data we show that gap can be rewritten free of the score activation matrix this gives us new insights about the estimation of the topic dictionary matrix by maximum marginal likelihood estimation in particular this explains the robustness of this estimator to over specified values of the factorization rank especially its ability to automatically prune irrelevant dictionary columns as empirically observed in previous work the marginalization of the activation matrix leads in turn to a new monte carlo expectation maximization algorithm with favorable properties","_deepnote_index_column":0},{"ID":20974,"TITLE":"Laboratory mid-IR spectra of equilibrated and igneous meteorites. Searching for observables of planetesimal debris","ABSTRACT":"  Meteorites contain minerals from Solar System asteroids with different\nproperties (like size, presence of water, core formation). We provide new\nmid-IR transmission spectra of powdered meteorites to obtain templates of how\nmid-IR spectra of asteroidal debris would look like. This is essential for\ninterpreting mid-IR spectra of past and future space observatories, like the\nJames Webb Space Telescope. We show that the transmission spectra of wet and\ndry chondrites, carbonaceous and ordinary chondrites and achondrite and\nchondrite meteorites are distinctly different in a way one can distinguish in\nastronomical mid-IR spectra. The two observables that spectroscopically\nseparate the different meteorites groups (and thus the different types of\nparent bodies) are the pyroxene-olivine feature strength ratio and the peak\nshift of the olivine spectral features due to an increase in the iron\nconcentration of the olivine.\n","TITLE_PROCESSED":"laboratory mid ir spectra of equilibrated and igneous meteorites searching for observables of planetesimal debris","ABSTRACT_PROCESSED":"meteorites contain minerals from solar system asteroids with different properties like size presence of water core formation we provide new mid ir transmission spectra of powdered meteorites to obtain templates of how mid ir spectra of asteroidal debris would look like this is essential for interpreting mid ir spectra of past and future space observatories like the james webb space telescope we show that the transmission spectra of wet and dry chondrites carbonaceous and ordinary chondrites and achondrite and chondrite meteorites are distinctly different in a way one can distinguish in astronomical mid ir spectra the two observables that spectroscopically separate the different meteorites groups and thus the different types of parent bodies are the pyroxene olivine feature strength ratio and the peak shift of the olivine spectral features due to an increase in the iron concentration of the olivine","_deepnote_index_column":1},{"ID":20975,"TITLE":"Case For Static AMSDU Aggregation in WLANs","ABSTRACT":"  Frame aggregation is a mechanism by which multiple frames are combined into a\nsingle transmission unit over the air. Frames aggregated at the AMSDU level use\na common CRC check to enforce integrity. For longer aggregated AMSDU frames,\nthe packet error rate increases significantly for the same bit error rate.\nHence, multiple studies have proposed doing AMSDU aggregation adaptively based\non the error rate. This study evaluates if there is a \\emph{practical}\nadvantage in doing adaptive AMSDU aggregation based on the link bit error rate.\nEvaluations on a model show that instead of implementing a complex adaptive\nAMSDU frame aggregation mechanism which impact queuing and other implementation\naspects, it is easier to influence packet error rate with traditional\nmechanisms while keeping the AMSDU aggregation logic simple.\n","TITLE_PROCESSED":"case for static amsdu aggregation in wlans","ABSTRACT_PROCESSED":"frame aggregation is a mechanism by which multiple frames are combined into a single transmission unit over the air frames aggregated at the amsdu level use a common crc check to enforce integrity for longer aggregated amsdu frames the packet error rate increases significantly for the same bit error rate hence multiple studies have proposed doing amsdu aggregation adaptively based on the error rate this study evaluates if there is a emph practical advantage in doing adaptive amsdu aggregation based on the link bit error rate evaluations on a model show that instead of implementing a complex adaptive amsdu frame aggregation mechanism which impact queuing and other implementation aspects it is easier to influence packet error rate with traditional mechanisms while keeping the amsdu aggregation logic simple","_deepnote_index_column":2},{"ID":20976,"TITLE":"The $Gaia$-ESO Survey: the inner disk intermediate-age open cluster NGC 6802","ABSTRACT":"  Milky Way open clusters are very diverse in terms of age, chemical\ncomposition, and kinematic properties. Intermediate-age and old open clusters\nare less common, and it is even harder to find them inside the solar\nGalactocentric radius, due to the high mortality rate and strong extinction\ninside this region. NGC 6802 is one of the inner disk open clusters (IOCs)\nobserved by the $Gaia$-ESO survey (GES). This cluster is an important target\nfor calibrating the abundances derived in the survey due to the kinematic and\nchemical homogeneity of the members in open clusters. Using the measurements\nfrom $Gaia$-ESO internal data release 4 (iDR4), we identify 95 main-sequence\ndwarfs as cluster members from the GIRAFFE target list, and eight giants as\ncluster members from the UVES target list. The dwarf cluster members have a\nmedian radial velocity of $13.6\\pm1.9$ km s$^{-1}$, while the giant cluster\nmembers have a median radial velocity of $12.0\\pm0.9$ km s$^{-1}$ and a median\n[Fe/H] of $0.10\\pm0.02$ dex. The color-magnitude diagram of these cluster\nmembers suggests an age of $0.9\\pm0.1$ Gyr, with $(m-M)_0=11.4$ and\n$E(B-V)=0.86$. We perform the first detailed chemical abundance analysis of NGC\n6802, including 27 elemental species. To gain a more general picture about\nIOCs, the measurements of NGC 6802 are compared with those of other IOCs\npreviously studied by GES, that is, NGC 4815, Trumpler 20, NGC 6705, and\nBerkeley 81. NGC 6802 shows similar C, N, Na, and Al abundances as other IOCs.\nThese elements are compared with nucleosynthetic models as a function of\ncluster turn-off mass. The $\\alpha$, iron-peak, and neutron-capture elements\nare also explored in a self-consistent way.\n","TITLE_PROCESSED":"the gaia eso survey the inner disk intermediate age open cluster ngc 6802","ABSTRACT_PROCESSED":"milky way open clusters are very diverse in terms of age chemical composition and kinematic properties intermediate age and old open clusters are less common and it is even harder to find them inside the solar galactocentric radius due to the high mortality rate and strong extinction inside this region ngc 6802 is one of the inner disk open clusters iocs observed by the gaia eso survey ges this cluster is an important target for calibrating the abundances derived in the survey due to the kinematic and chemical homogeneity of the members in open clusters using the measurements from gaia eso internal data release 4 idr4 we identify 95 main sequence dwarfs as cluster members from the giraffe target list and eight giants as cluster members from the uves target list the dwarf cluster members have a median radial velocity of 13 6 pm1 9 km s 1 while the giant cluster members have a median radial velocity of 12 0 pm0 9 km s 1 and a median fe h of 0 10 pm0 02 dex the color magnitude diagram of these cluster members suggests an age of 0 9 pm0 1 gyr with m m _0 11 4 and e b v 0 86 we perform the first detailed chemical abundance analysis of ngc 6802 including 27 elemental species to gain a more general picture about iocs the measurements of ngc 6802 are compared with those of other iocs previously studied by ges that is ngc 4815 trumpler 20 ngc 6705 and berkeley 81 ngc 6802 shows similar c n na and al abundances as other iocs these elements are compared with nucleosynthetic models as a function of cluster turn off mass the alpha iron peak and neutron capture elements are also explored in a self consistent way","_deepnote_index_column":3},{"ID":20977,"TITLE":"Witness-Functions versus Interpretation-Functions for Secrecy in Cryptographic Protocols: What to Choose?","ABSTRACT":"  Proving that a cryptographic protocol is correct for secrecy is a hard task.\nOne of the strongest strategies to reach this goal is to show that it is\nincreasing, which means that the security level of every single atomic message\nexchanged in the protocol, safely evaluated, never deceases. Recently, two\nfamilies of functions have been proposed to measure the security level of\natomic messages. The first one is the family of interpretation-functions. The\nsecond is the family of witness-functions. In this paper, we show that the\nwitness-functions are more efficient than interpretation-functions. We give a\ndetailed analysis of an ad-hoc protocol on which the witness-functions succeed\nin proving its correctness for secrecy while the interpretation-functions fail\nto do so.\n","TITLE_PROCESSED":"witness functions versus interpretation functions for secrecy in cryptographic protocols what to choose","ABSTRACT_PROCESSED":"proving that a cryptographic protocol is correct for secrecy is a hard task one of the strongest strategies to reach this goal is to show that it is increasing which means that the security level of every single atomic message exchanged in the protocol safely evaluated never deceases recently two families of functions have been proposed to measure the security level of atomic messages the first one is the family of interpretation functions the second is the family of witness functions in this paper we show that the witness functions are more efficient than interpretation functions we give a detailed analysis of an ad hoc protocol on which the witness functions succeed in proving its correctness for secrecy while the interpretation functions fail to do so","_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"      ID                                              TITLE  \\\n0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n2  20975         Case For Static AMSDU Aggregation in WLANs   \n3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n4  20977  Witness-Functions versus Interpretation-Functi...   \n\n                                            ABSTRACT  \\\n0    We present novel understandings of the Gamma...   \n1    Meteorites contain minerals from Solar Syste...   \n2    Frame aggregation is a mechanism by which mu...   \n3    Milky Way open clusters are very diverse in ...   \n4    Proving that a cryptographic protocol is cor...   \n\n                                     TITLE_PROCESSED  \\\n0  closed form marginal likelihood in gamma poiss...   \n1  laboratory mid ir spectra of equilibrated and ...   \n2         case for static amsdu aggregation in wlans   \n3  the gaia eso survey the inner disk intermediat...   \n4  witness functions versus interpretation functi...   \n\n                                  ABSTRACT_PROCESSED  \n0  we present novel understandings of the gamma p...  \n1  meteorites contain minerals from solar system ...  \n2  frame aggregation is a mechanism by which mult...  \n3  milky way open clusters are very diverse in te...  \n4  proving that a cryptographic protocol is corre...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TITLE</th>\n      <th>ABSTRACT</th>\n      <th>TITLE_PROCESSED</th>\n      <th>ABSTRACT_PROCESSED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20973</td>\n      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n      <td>We present novel understandings of the Gamma...</td>\n      <td>closed form marginal likelihood in gamma poiss...</td>\n      <td>we present novel understandings of the gamma p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20974</td>\n      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n      <td>Meteorites contain minerals from Solar Syste...</td>\n      <td>laboratory mid ir spectra of equilibrated and ...</td>\n      <td>meteorites contain minerals from solar system ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20975</td>\n      <td>Case For Static AMSDU Aggregation in WLANs</td>\n      <td>Frame aggregation is a mechanism by which mu...</td>\n      <td>case for static amsdu aggregation in wlans</td>\n      <td>frame aggregation is a mechanism by which mult...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20976</td>\n      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n      <td>Milky Way open clusters are very diverse in ...</td>\n      <td>the gaia eso survey the inner disk intermediat...</td>\n      <td>milky way open clusters are very diverse in te...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20977</td>\n      <td>Witness-Functions versus Interpretation-Functi...</td>\n      <td>Proving that a cryptographic protocol is cor...</td>\n      <td>witness functions versus interpretation functi...</td>\n      <td>proving that a cryptographic protocol is corre...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"predictions = NB_pipeline.predict(df_test[['TITLE_PROCESSED', 'ABSTRACT_PROCESSED']])","metadata":{"tags":[],"cell_id":"00014-7936af33-6ab1-482b-80ea-0c43b01e17a9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions","metadata":{"tags":[],"cell_id":"00017-6865af6e-2daa-487c-82e2-2e50a07eb742"},"outputs":[{"output_type":"execute_result","execution_count":116,"data":{"text/plain":"array([[0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       ...,\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0, 0]])"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"df_predictions = pd.DataFrame(predictions, index=df_test.ID, columns=[\n    'Computer Science', 'Physics', 'Mathematics',\n    'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n)\ndf_predictions.head()","metadata":{"tags":[],"cell_id":"00016-4be64342-006f-4586-ad66-4d0ca21d64e2"},"outputs":[{"output_type":"execute_result","execution_count":119,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":6,"columns":[{"name":"Computer Science","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"Physics","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"Mathematics","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Statistics","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"Quantitative Biology","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Quantitative Finance","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"Computer Science":0,"Physics":0,"Mathematics":0,"Statistics":1,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":20973},{"Computer Science":0,"Physics":1,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":20974},{"Computer Science":1,"Physics":0,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":20975},{"Computer Science":0,"Physics":1,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":20976},{"Computer Science":1,"Physics":0,"Mathematics":0,"Statistics":0,"Quantitative Biology":0,"Quantitative Finance":0,"_deepnote_index_column":20977}],"rows_bottom":null},"text/plain":"       Computer Science  Physics  Mathematics  Statistics  \\\nID                                                          \n20973                 0        0            0           1   \n20974                 0        1            0           0   \n20975                 1        0            0           0   \n20976                 0        1            0           0   \n20977                 1        0            0           0   \n\n       Quantitative Biology  Quantitative Finance  \nID                                                 \n20973                     0                     0  \n20974                     0                     0  \n20975                     0                     0  \n20976                     0                     0  \n20977                     0                     0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Computer Science</th>\n      <th>Physics</th>\n      <th>Mathematics</th>\n      <th>Statistics</th>\n      <th>Quantitative Biology</th>\n      <th>Quantitative Finance</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20973</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20974</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20975</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20976</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20977</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"df_predictions.to_csv('predictions_v1.csv', index_label='ID')","metadata":{"tags":[],"cell_id":"00017-f631dfa6-72e9-46ff-87e7-b82fddc81be4"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"70e4094a-fc12-469b-a1b0-98cab7770acb","deepnote_execution_queue":[]}}